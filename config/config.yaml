# General settings
project:
  name: "SELF"
  version: "1.0.0"

# Data settings
data:
  dataset_path: "data/"
  batch_size: 32

# Output settings
outputs:
  base_folder: "outputs"
  plots_folder: "plots"
  results_folder: "results"
  weights_folder: "weights"


# Model settings
pythia160m:
  name: "Pythia-160M"
  type: "decoder"
  num_layers: 12
  naming_conv:
    base: "gpt_neox.layers."
    layers: 
      #block1: "input_layernorm.weight"
      #block2: "post_attention_layernorm.weight"
      block1: "attention.query_key_value.weight"
      block2: "attention.dense.weight"
      block3: "mlp.dense_h_to_4h.weight"
      block4: "mlp.dense_4h_to_h.weight"


pythia410m:
  name: "Pythia-410M"
  type: "decoder"
  num_layers: 24
  naming_conv:
    base: "gpt_neox.layers."
    layers: 
      #block1: "input_layernorm.weight"
      #block2: "post_attention_layernorm.weight"
      block1: "attention.query_key_value.weight"
      block2: "attention.dense.weight"
      block3: "mlp.dense_h_to_4h.weight"
      block4: "mlp.dense_4h_to_h.weight"

gptj: 
  name: "GPT-J"
  type: "decoder"
  num_layers: 28
  naming_conv:
    base: "transformer.h."
    layers: 
      block1: ".attn.k_proj.weight"
      block2: ".attn.q_proj.weight"
      block3: ".attn.v_proj.weight"
      block4: ".attn.out_proj.weight"
      block5: ".mlp.fc_in.weight"
      block6: ".mlp.fc_out.weight"


roberta_base:
  name: "RoBERTa_base"
  type: "encoder_decoder"
  num_layers: 12
  naming_conv:
    base: "roberta.encoder.layer."
    layers: 
      block1: "attention.self.key.weight"
      block2: "attention.self.query.weight"
      block3: "attention.self.value.weight"
      block4: "attention.output.dense.weight"
      block5: "intermediate.dense.weight"
      block6: "output.dense.weight"


roberta_large:
  name: "RoBERTa_large"
  type: "encoder_decoder"
  num_layers: 24
  naming_conv:
    base: "roberta.encoder.layer."
    layers: 
      block1: "attention.self.key.weight"
      block2: "attention.self.query.weight"
      block3: "attention.self.value.weight"
      block4: "attention.output.dense.weight"
      block5: "intermediate.dense.weight"
      block6: "output.dense.weight"

Arguments:
  lname:
    arg: "Layer name"
    values:
      k_proj: "Attention (key)"
      q_proj: "Attention (query)"
      v_proj: "Attention (value)"
      out_proj: "Attention (out)"
      fc_in: "MLP input"
      fc_out: "MLP output"
  lnum:
    arg: "Layer number"
    values: "num"
  rate:
    arg: "Reduction rate"
    values: "num"
  prop_data:
    arg: "Proportion of dataset"
    values: "num"
  dataset:
    arg: "Evaluation dataset"
    values:
      counterfact: "Counterfact"
  model:
    arg: "Model"
    values: 
      roberta: Roberta
      pythia: Pythia
  k:
    arg: "Top-k for evaluation"
    values: "num"
  batch_size:
    arg: "Batch size"
    values: "num"
  intervention:
    arg: "Type of intervention"
    values:
      lr: "Low rank approximation"
      mm: "Monarch matrix approximation"



